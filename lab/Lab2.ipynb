{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "# Machine learning\n",
    "\n",
    "Let's pick up from where we left in the last lab. By the end of the lab you found interesting genes that were **differentially expressed** between two **clinically relevant** conditions.\n",
    "\n",
    "(Note: the selection of relevant clinical conditions is even more important in this lab, if you haven't spent some time on it previously, take the time now.)\n",
    "\n",
    "You've also learned that one way to make more sense of these results is by creating **machine learning modells** to distinguish between different groups/conditions. Let's try that.\n",
    "\n",
    "As always, we start by importing relevant libraries and loading the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import qvalue\n",
    "\n",
    "from ipywidgets import interact, interact_manual\n",
    "from ipywidgets import IntSlider, FloatSlider, Dropdown, Text\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "interact_enrich=interact_manual.options(manual_name=\"Enrichment analysis\")\n",
    "interact_plot=interact_manual.options(manual_name=\"Plot\")\n",
    "interact_calc=interact_manual.options(manual_name=\"Calculate tests\")\n",
    "\n",
    "interact_gen=interact_manual.options(manual_name=\"Initialize data\")\n",
    "interact_SVM=interact_manual.options(manual_name=\"Train SVM\")\n",
    "\n",
    "clinical_data = pd.read_csv('../data/brca_clin.tsv.gz', sep ='\\t', index_col=2)\n",
    "clinical_data = clinical_data.iloc[4:,1:]\n",
    "expression_data = pd.read_csv('../data/brca.tsv.gz', sep ='\\t', index_col=1)\n",
    "expression_data = expression_data.iloc[:,2:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "# 1 Support Vector Machine\n",
    "\n",
    "We continue with our dataset on breast cancer and we will try to use machine learning to **predict the clinical conditions based on the gene expression data**.\n",
    "\n",
    "You will be able two choose the two clinical groups we will use to train the model, and your choice will affect the results and performance of the model. Special attention should be paid to the number of data points in each group, which will be displayed after you select the groups.\n",
    "\n",
    "Let's train our first SVM. You've learned the merits of feature scaling (or in other terms, normalizing the input), and so we will test it.\n",
    "Below you will separate the data set into two, and train a SVM to distinguish between the two using the gene expression values of the samples. You can choose whether you'd like to rescale (normalize) the features first or not.\n",
    "\n",
    "Once you have done so, you will be presented with a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), and from that will need to devise a performance metric.\n",
    "[This Wikipedia page](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) can be helpful when choosing how to measure the performance. You might want to do that before running the SVM tests.\n",
    "\n",
    "You know the drill by now, play around with it for a while and answer the questions.\n",
    "\n",
    "Interactive fields:\n",
    "* **Rescale**: whether to apply mean and variance normalization before training the SVM.\n",
    "* **Data_split**: the fraction of the data that is left on the test set.\n",
    "* **Max_iterations**: the maximum number of iterations the SVM is allowed to perform, increase it if you are seeing convergence errors.\n",
    "\n",
    "## Questions\n",
    "\n",
    "\n",
    "* 1.1 Which clinical groups have you decided to classify using SVMs? Why did you think these are the groups which could be classified using SVMs and gene expression data? Give 1-2 arguments to justify your choice.\n",
    "* 1.2 Report the size of your two groups. Which performance metric did you choose? Give 1-2 arguments to justify your choice.\n",
    "* 1.3 How much did you chose to leave out for the test set? Justify your choice as an optimal tradeoff between the performance of your classifier and your ability to measure that performance? (What would happen to the performance and your ability to measure it if you would exclude less/more data from the training?)\n",
    "* 1.4 Did you achieve better performance by normalizing the gene expression? Provide 1-2 plausable reasons why the performance improved or did not improve.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(clinical_df, expression_df, separator, cond1, cond2):\n",
    "    try:\n",
    "        group1 = clinical_df[separator] == cond1\n",
    "        index1 = clinical_df[group1].index\n",
    "        group2 = clinical_df[separator] == cond2\n",
    "        index2 = clinical_df[group2].index\n",
    "    except:\n",
    "        print('Clinical condition wrong')\n",
    "    expression1 = expression_df.loc[index1].dropna()\n",
    "    expression2 = expression_df.loc[index2].dropna()\n",
    "    expression = pd.concat([expression1, expression2])\n",
    "    X = expression.values\n",
    "    y = np.append(np.repeat(0, len(expression1)), np.repeat(1, len(expression2)))\n",
    "    display(pd.DataFrame([len(index1),len(index2)], columns = ['Number of points'], index = ['Group 1', 'Group 2']))\n",
    "    return X, y\n",
    "\n",
    "def train_SVM(X, y, C=1, scale = False, max_iter = 1000):\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    clf = LinearSVC(C=C, max_iter=max_iter)\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "def print_accuracy(X_train, y_train, X_test, y_test, clf, scale=False):\n",
    "    if scale:\n",
    "        train_size = X_train.shape[0]\n",
    "        scaler = StandardScaler()\n",
    "        X = np.concatenate((X_train, X_test))\n",
    "        X = scaler.fit_transform(X)\n",
    "        X_train = X[:train_size,:]\n",
    "        X_test = X[train_size:,:] \n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    ac_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    ac_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "    display(pd.DataFrame(np.concatenate((ac_matrix_train,ac_matrix_test), axis =1), columns = [\"predicted G1 (training)\",\"predicted G2 (training)\", \"predicted G1 (test)\",\"predicted G2 (test)\"],index=[\"actual G1\",\"actual G2\"]))\n",
    "    \n",
    "def plot_pca_variance(X, ncomp = 1, scale = False):\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    sns.set(style='darkgrid', context='talk')\n",
    "    plt.plot(np.arange(1,len(pca.explained_variance_ratio_)+1),np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    \n",
    "    plt.vlines(ncomp, 0, plt.gca().get_ylim()[1], color='r', linestyles = 'dashed')\n",
    "    h = np.cumsum(pca.explained_variance_ratio_)[ncomp -1]\n",
    "    plt.hlines(h, 0, plt.gca().get_xlim()[1], color='r', linestyles = 'dashed')\n",
    "    plt.title(str(ncomp) + ' components, ' + str(round(h, 3)) + ' variance explained')\n",
    "    plt.show()\n",
    "    \n",
    "def reduce_data(X, n, scale = False):\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=n)\n",
    "    Xr = pca.fit_transform(X)\n",
    "    return Xr\n",
    "\n",
    "def interact_split_data(Criteria, Group_1, Group_2):\n",
    "    global BRCA_X, BRCA_y\n",
    "    BRCA_X, BRCA_y = split_data(clinical_data, expression_data, Criteria, Group_1, Group_2)\n",
    "\n",
    "def interact_SVM_1(Rescale, Data_split, Max_iterations):\n",
    "    if Rescale:\n",
    "        X = scale_data(BRCA_X)\n",
    "    else:\n",
    "        X = BRCA_X\n",
    "    max_iter = int(Max_iterations)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(BRCA_X, BRCA_y, test_size=Data_split)\n",
    "    clf = train_SVM(X_train, y_train, C=1, scale = Rescale, max_iter=max_iter)\n",
    "    print_accuracy(X_train, y_train, X_test, y_test, clf, scale = Rescale)\n",
    "    \n",
    "interact_gen(interact_split_data, Criteria=Text('Surgical procedure first'), Group_1 = Text('Simple Mastectomy'), Group_2=Text('Lumpectomy'))\n",
    "interact_SVM(interact_SVM_1, Rescale = False, Data_split = FloatSlider(min=0,max=1,value=0.1, step = 0.05), Max_iterations = Text('1000'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regularization\n",
    "\n",
    "You have learned about the propensity of machine learning methods to overfit.\n",
    "One way of trying to avoid overfitting is called [**regularization**](https://en.wikipedia.org/wiki/Regularization_(mathematics)).\n",
    "Regularization, as the names suggests, is a way of adding extra rules in a way that adds extra information on the model. It also serves as a great way to add **domain knowledge** into a general model.\n",
    "\n",
    "SVMs have a built in regularization mechanism in the form of the **C parameter**. If you are not familiar with this parameter, read a bit on it, and then play around with the SVM below.\n",
    "As with every form of regularization, it is more of an art than a science, and so a good value will depend a lot on how your data looks like.\n",
    "\n",
    "New interactive fields:\n",
    "* **C_parameter**: the value of the C parameter.\n",
    "\n",
    "## Questions\n",
    "\n",
    "\n",
    "* 2.1 What is the \"extra rule\" that regularization with C parameter adds to the algorithm?\n",
    "* 2.2 How would the trust you can have in labelling used for training correspond to the value of the C parameter you would choose? Think of hard to clasify/borderline, and provide one hypothetical example when a high C value would be preferabele, and one when a low one would be better.\n",
    "* 2.3 Did regularization with C parameter improve the performance of your predictor from part 3? Provide one reason why you think it was (not) so.\n",
    "* 2.4 Maximizing the perfomance of the classifier is not always the best criterium for deciding whether to regularize or not. Provide one hypothetical example when you would set a C value based on **prior knowledge** rather than on **best performance**. Hint: consider the data available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_SVM_2(Rescale, Data_split, Max_iterations, C_parameter):\n",
    "    max_iter = int(Max_iterations)\n",
    "    C = float(C_parameter)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(BRCA_X, BRCA_y, test_size=Data_split)\n",
    "    clf = train_SVM(X_train, y_train, C=C, scale = Rescale, max_iter=max_iter)\n",
    "    print_accuracy(X_train, y_train, X_test, y_test, clf, scale = Rescale)\n",
    "\n",
    "interact_gen(interact_split_data, Criteria=Text('Surgical procedure first'), Group_1 = Text('Simple Mastectomy'), Group_2=Text('Lumpectomy'))\n",
    "interact_SVM(interact_SVM_2, Rescale = False, Data_split = FloatSlider(min=0,max=1,value=0.1, step = 0.05), Max_iterations = Text('1000'), C_parameter = Text('1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Dimensionality\n",
    "\n",
    "Let's explore how the dimensionality of the data affects the prediction accuracy. You have heard of the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) and, in bioinformatics, often we are subject to it just because of the amount of genes on a genome and the low number of samples.\n",
    "\n",
    "Luckily, you have also learned at least one way to reduce the dimensionality of the data. Here we will use PCA to reduce the dimensions from around 20 thousand genes (dimensions) to a more manageable number of dimensions.\n",
    "First, you will need to choose if you normalize all the dimensions before performing the PCA, then you will be shown how much of the variance in the data is captured by the first components (remember that the normalization also changes the variance). With this information you will be able to have an educated guess on how many principal components you want to use to train the SVM.\n",
    "\n",
    "The core PCA algorithm, as such, does not reduce the number of dimensions. It levarages co-variation of different variables (in different dimensions) to capture as much of total variations in as few dimensions, i.e. principal components, as possible. In contrast to dimensionality reduction methods such as UMAP or tSNE, no information is lost. Thus, given the full output matrices from PCA, it should be possible to retrive original values (check Singular Value Decomposition, e.g. in the lecture about PCA, for more details). After the transformation, the samples should vary a lot across the first few components, and little across all the rest. \n",
    "\n",
    "In most cases, PCA is used to retrive only a few first principal components, most often just the first two. The idea is that the first components should correspond to meaningful variance, driven by the the same (biological) processes in multiple samples, thus leading to co-variation. The other components can be, for some purposes, disregarded as \"noise\". By picking only few first components, PCA can be used for dimensionality reduction. That is very useful for visualization, as one can often show in 2D most of the variation corresponding to effects of key factors structuring the data. Another application can be to reduce the number of dimensions to avoid the curse of dimensionality, e.g. to improve the performance of an SVM-based predictor. This is what we will try to do in this exercise.\n",
    "\n",
    "As always, play with it as much as you need to answer the question.\n",
    "\n",
    "New interactive fields:\n",
    "\n",
    "* **PCA_scaling**: whether to apply the normalization before performing the PCA.\n",
    "* **N_components**: number of principal components selected.\n",
    "* **Reduce_dim**: whether to reduce the dimentions of the data before training the SVM.\n",
    "\n",
    "## Questions\n",
    "\n",
    "* 3.1 Explain one of the causes of **curse of dimensionality**. Why adding more information doesn't always increase the performance? It can be a characteristic of biological data/systems or a general property of the algorithms prone to the problem.\n",
    "* 3.2 After performing PCA on our data, how much of the variance did the first two components explain? What was the minimal number of components which explained most of the variation?\n",
    "* 3.3 Would you use the first two principal components to visualize the data? Give one reason why/why not.\n",
    "* 3.4 Did the dimensionality reduction improve the perfomance of the predictor? Provide parameters you have used and give the one cause you find most probable/influencial to explain the fact the performance did/did not improve? Could you have predicted the outcome based on the answers to p. 5.2 and the plot of explained variance vs the number of components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_PCA_plot(PCA_scaling, N_components):\n",
    "    n_comp = int(N_components)\n",
    "    plot_pca_variance(BRCA_X, scale=PCA_scaling, ncomp = n_comp)\n",
    "    \n",
    "def interact_SVM_3(Rescale, Data_split, Max_iterations, C_parameter, Reduce_dim, PCA_scaling, N_components):\n",
    "    max_iter = int(Max_iterations)\n",
    "    n_comp = int(N_components)\n",
    "    C = float(C_parameter)\n",
    "    if Reduce_dim:\n",
    "        X = reduce_data(BRCA_X, n = n_comp, scale=PCA_scaling)\n",
    "    else:\n",
    "        X = BRCA_X\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, BRCA_y, test_size=Data_split)\n",
    "    clf = train_SVM(X_train, y_train, C=C,  scale = Rescale, max_iter=max_iter)\n",
    "    print_accuracy(X_train, y_train, X_test, y_test, clf,  scale = Rescale)\n",
    "    \n",
    "interact_gen(interact_split_data, Criteria=Text('ER Status By IHC'), Group_1 = Text('Positive'), Group_2=Text('Negative'))\n",
    "interact_plot(interact_PCA_plot, PCA_scaling = False, N_components = Text('1'))\n",
    "interact_SVM(interact_SVM_3, Rescale = False, Data_split = FloatSlider(min=0,max=1,value=0.1, step = 0.05), Max_iterations = Text('1000'), C_parameter = Text('1'), Reduce_dim = False, PCA_scaling = False, N_components = Text('1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "**Note**: To get full points in the **bonus part** you only have to answer 2 out of the 3 questions below, but you **have to specify** on your report which question you are leaving out.\n",
    "\n",
    "## Bias-variance trade-off\n",
    "\n",
    "Bias and variance are both sources of error in any predictive model. The bias-variance trade-off principle states that usually models with high bias error have low variance error, and vice-versa.\n",
    "\n",
    "* B1.1 Read more about **bias** and **variance**. Provide a one sentence explanation of each of the terms, in your own words.\n",
    "* B1.2 Imgagine a real-life example of a classifier. Explain what you would like to classify. Would high bias affect the perdictor's ability to infer the trait you are interested in for new data? (I.e. What kind of new datapoints/samples the predictor would be most likely to missclasify?) And how would it be affected by high variance.\n",
    "* B1.3 Explain why there is a trade-off between bias and variance in context of complexity and overfitting of a model. Use figures if think it's useful.\n",
    "* B1.4 Provide one way to control the trade-off in SVMs and one in clustering.\n",
    "\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "Some times you don't have enough data, and it is a shame to leave some of it out of the training set. One way to mitigate this is by using cross validation.\n",
    "\n",
    "* B2.1 Implement cross validation in one of the SVMs you used above. In how many parts you've divided your set? Why?\n",
    "* B2.2 Compare the results above with the results from previous classifiers in this lab. Do you achieve better performance?\n",
    "\n",
    "## Parameter optimization\n",
    "\n",
    "For the questions below, we will explore more in depth how some parameter affect the performance by doing [parameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization).\n",
    "\n",
    "Perform a parameter optimization, using cross validation, for the following parameter on an SVM you used above. You can use ready functions such as [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to help. Plot and interpret the results for optimizing:\n",
    "\n",
    "* B3.1 Number of dimensions of the input (principal components)\n",
    "* B3.2 The C parameter\n",
    "* B3.3 The C parameter and number of dimensions, and scaling (of the input) at the same time.\n",
    "* B3.4 Comment about the relationship between C parameter and number of dimensions by using the results above.\n",
    "* B3.5 Now, investigate the claim that cross validation shouldn't be used to report performance. You may want to use [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1397873/) to do so. In your own words, provide one argument for using cross validation to report performance, one against, and state your opinion on the matter. (i.e. Would you use cross validation to report performance in your own research?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
