{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised machine learning on the TCGA Breast Cancer set\n",
    "\n",
    "This notebook can be run locally or on a remote cloud computer by clicking the badge below:\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/statisticalbiotechnology/cb2030/master?filepath=nb%2Fsupervised%2FsupervisedML.ipynb) or [![CoLab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/statisticalbiotechnology/cb2030/blob/master/nb/supervised/supervisedML.ipynb)\n",
    "\n",
    "We begin by reading in the TCGA Breast Cancer dataset, and calculate significance of the measured genes as differntial expressed when comparing Progesterone Positive with Progesterone Negative cancers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    ![ ! -f \"cb2030/README.md\" ] && git clone https://github.com/statisticalbiotechnology/cb2030.git\n",
    "    my_path = \"cb2030/nb/\"\n",
    "else:\n",
    "    my_path = \"../\"\n",
    "sys.path.append(my_path) # Read local modules for tcga access and qvalue calculations\n",
    "import tcga_read as tcga\n",
    "import qvalue\n",
    "\n",
    "brca = tcga.get_expression_data(\"../../data/brca.tsv.gz\", 'http://download.cbioportal.org/brca_tcga_pub2015.tar.gz',\"data_RNA_Seq_v2_expression_median.txt\")\n",
    "brca_clin = tcga.get_clinical_data(\"../../data/brca_clin.tsv.gz\", 'http://download.cbioportal.org/brca_tcga_pub2015.tar.gz',\"data_clinical_sample.txt\")\n",
    "brca.dropna(axis=0, how='any', inplace=True)\n",
    "brca = brca.loc[~(brca<=0.0).any(axis=1)]\n",
    "brca = pd.DataFrame(data=np.log2(brca),index=brca.index,columns=brca.columns)\n",
    "brca_clin.loc[\"PR\"]= (brca_clin.loc[\"PR status by ihc\"]!=\"Negative\") \n",
    "pr_bool = (brca_clin.loc[\"PR\"] == True)\n",
    "\n",
    "\n",
    "def get_significance_two_groups(row):\n",
    "    log_fold_change = row[pr_bool].mean() - row[~pr_bool].mean()\n",
    "    p = ttest_ind(row[pr_bool],row[~pr_bool],equal_var=False)[1]\n",
    "    return [p,-np.log10(p),log_fold_change]\n",
    "\n",
    "pvalues = brca.apply(get_significance_two_groups,axis=1,result_type=\"expand\")\n",
    "pvalues.rename(columns = {list(pvalues)[0]: 'p', list(pvalues)[1]: '-log_p', list(pvalues)[2]: 'log_FC'}, inplace = True)\n",
    "qvalues = qvalue.qvalues(pvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The overoptimstic investigator\n",
    "\n",
    "We begin with a case of supervised machine learning aimed as a warning, as it illustrates the importance of separating training from testing data.\n",
    "\n",
    "Imagine a situation where we want to find the best combination of genes unrelated to a condition that still are telling of the condition. Does that sound like an imposibility, it is because it is imposible. However, there is nothing stopping us to try.\n",
    "\n",
    "So first we select the 1000 genes which are the least differentialy expressed genes when comparing PR positive with PR negative breast cancers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last1k=brca.loc[qvalues.iloc[-1000:,:].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently we standardize the data, i.e. we assure a standard deviation of 1 and a mean of zero for every gene among our 1k genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(last1k.values.T)  # Scale all gene expression values to stdv =1 and mean =0\n",
    "y = 2*pr_bool.values.astype(int) - 1       # transform from bool to -1 and 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to try to train a linear SVM for the task of predictin PR negatives from PR positives. We test the performance of our classifier on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "clf = svm.LinearSVC(C=1,max_iter=5000).fit(X, y)  # Train a SVM\n",
    "y_pred = clf.predict(X)                        # Predict labels for the give features\n",
    "pd.DataFrame(data = confusion_matrix(y, y_pred),columns = [\"predicted_PR-\",\"predicted_PR+\"],index=[\"actual_PR-\",\"actualPR+\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! The classifier manage to use junk data to perfectly separate our PR+ from PR- cancers. \n",
    "\n",
    "However, before we call NEJM, lets try to see if we can sparate an *independent* test set in the same manner. We use the function train_test_split to divide the data into 60% training data and 40% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0)\n",
    "clf = svm.LinearSVC(C=1,max_iter=5000).fit(X_train, y_train) # Train an SVM\n",
    "y_pred = clf.predict(X_test)                              # Predict labels for the give features\n",
    "pd.DataFrame(data = confusion_matrix(y_test, y_pred),columns = [\"predicted_PR-\",\"predicted_PR+\"],index=[\"actual_PR-\",\"actualPR+\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting, the classifier seems to have very little predictive power.  \n",
    "\n",
    "The reason for the discrepency of the two predictors are that in both cases the large number of variables makes the predictor to overfit to the data. In the first instance, we could not detect the problem as we were testing on the overfitted data. However, when holding out a separate test set, the predictors weak performance was blatantly visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A low dimensional classifier\n",
    "\n",
    "Lets now focus on an alternative setting, where we instead select six separate genes which are among the most differentially expressed transcripts when comparing PR+ and PR-.\n",
    "\n",
    "How would we combine their expression values optimaly? \n",
    "\n",
    "Again we begin by standardize our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top6=brca.loc[qvalues.iloc[[1,2,5,6,9],:].index]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(top6.values.T) # Scale all gene expression values to stdv =1 and mean =0\n",
    "y = 2*pr_bool.values.astype(int) - 1           # transform from bool to -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then separate 40% of our cancers into a separate test set. The function $GridSearchCV$ use cross validation (k=5) to select an optimal slack penalty $C$ out from a vector of differnt choices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "param_grid = [{'C': [0.0001, 0.001, 0.1, 1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(svm.LinearSVC(max_iter=10000000,class_weight=\"balanced\"), param_grid, cv=5, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best cross validation accuracy for the model: \" + str(clf.best_params_))\n",
    "y_pred = clf.predict(X_test)\n",
    "pd.DataFrame(data = confusion_matrix(y_test, y_pred),columns = [\"predicted_PR-\",\"predicted_PR+\"],index=[\"actual_PR-\",\"actualPR+\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the choise of penalty $C=0.1$, we can now perform a cross validation (k=5) on the full data set. Here we will train thee separate classifiers on ech cross validation training set, and subsequently merge each such predictor's prediction into one combined result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "y_pred, y_real = np.array([]), np.array([])\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_id, test_id in skf.split(X, y):\n",
    "    X_train, X_test, y_train, y_test = X[train_id,:], X[test_id,:], y[train_id],y[test_id]\n",
    "    clf = svm.LinearSVC(C=0.1,max_iter=100000).fit(X_train, y_train) # Train an SVM\n",
    "    y_pred_fold = clf.predict(X_test)                                # Predict labels for the give features\n",
    "    y_pred = np.concatenate([y_pred,y_pred_fold])\n",
    "    y_real = np.concatenate([y_real,y_test])\n",
    "pd.DataFrame(data = confusion_matrix(y_real, y_pred),columns = [\"predicted_PR-\",\"predicted_PR+\"],index=[\"actual_PR-\",\"actualPR+\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
